{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "federated learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0GJS5rzNK2kqdENXBBGia",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guhang987/federated-learning/blob/master/fl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sotZxUgvOL3B",
        "outputId": "45c9c78d-f972-4f77-c10b-035eb3c9c8b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "!pip install --quiet --upgrade tensorflow_federated_nightly\n",
        "!pip install --quiet --upgrade nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import collections\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "# TODO(b/148678573,b/148685415): must use the reference context because it\n",
        "# supports unbounded references and tff.sequence_* intrinsics.\n",
        "tff.backends.reference.set_reference_context()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 512kB 3.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 393.5MB 40kB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 51.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 44.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 44.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 51.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 45.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.6MB 45.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 46.2MB/s \n",
            "\u001b[?25h  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-nightly 2.4.0.dev20201020 has requirement absl-py~=0.10, but you'll have absl-py 0.9.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-nightly 2.4.0.dev20201020 has requirement grpcio~=1.32.0, but you'll have grpcio 1.29.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-nightly 2.4.0.dev20201020 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:43: UserWarning: You are currently using a nightly version of TensorFlow (2.4.0-dev20201020). \n",
            "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
            "If you encounter a bug, do not file an issue on GitHub.\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Uok1cUSshkN",
        "outputId": "8a7bd4e0-4bfe-4ee5-bf8b-5574feb7a5b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "# %load_ext tensorboard\n",
        "# logdir = \"/tmp/logs/scalars/training/\"\n",
        "# summary_writer = tf.summary.create_file_writer(logdir)\n",
        "\n",
        "#with summary_writer.as_default():\n",
        "  #for round_num in range(1, NUM_ROUNDS):\n",
        "    #tf.summary.scalar(name, value, step=round_num)\n",
        "\n",
        "#%tensorboard --logdir /tmp/logs/scalars/ --port=0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0-dev20201020'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usbrIMi0nNPp",
        "outputId": "2571109e-5042-437b-e3b6-a1d61f637845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
        "import copy\n",
        "#深拷贝\n",
        "ordered_mnist_train=copy.deepcopy(mnist_train)\n",
        "#给mnist_train按标签排序\n",
        "f=0\n",
        "for digit in range(10):\n",
        "  index=[i for i,d in enumerate(mnist_train[1]) if d==digit]\n",
        "  for i in range(5420):\n",
        "    ordered_mnist_train[0][f+i]=mnist_train[0][index[i]]\n",
        "    ordered_mnist_train[1][f+i]=mnist_train[1][index[i]]\n",
        "  f+=5420"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS4T32eau-E2"
      },
      "source": [
        "#检测排序结果\n",
        "from matplotlib import pyplot as plt\n",
        "for i in range(5420*3-1,5420*3+1):\n",
        "  plt.imshow(np.array(ordered_mnist_train[0][i]), cmap='gray', aspect='equal')\n",
        "  plt.grid(False)\n",
        "  print(ordered_mnist_train[1][i])\n",
        "  _ = plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3Zb9QczOTRV"
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "# 定义：20个shard 每个shard包含271张图片\n",
        "SHARD_SIZE = 271\n",
        "def get_data(source,i,j):\n",
        "  device=[]\n",
        "  #根据普通索引ij计算对应shard数目，继而计算对应图片索引\n",
        "  shard1=(20*j+i)*SHARD_SIZE\n",
        "  shard2=(j+20*i+10)*SHARD_SIZE\n",
        "  #600张图片索引存放在all_samples中\n",
        "  all_samples=list(range(shard1,shard1+SHARD_SIZE))+list(range(shard2,shard2+SHARD_SIZE))\n",
        "\n",
        "  batch_nums=int(2*SHARD_SIZE/BATCH_SIZE)\n",
        "  #分成batch_nums个batch\n",
        "  for i in range(int(batch_nums)):\n",
        "    device_index=all_samples[BATCH_SIZE*i:BATCH_SIZE*i + BATCH_SIZE]\n",
        "    device.append({\n",
        "        'x':\n",
        "            np.array([source[0][i].flatten() / 255.0 for i in device_index],\n",
        "                    dtype=np.float32),\n",
        "        'y':\n",
        "            np.array([source[1][i] for i in device_index], dtype=np.int32)\n",
        "    })\n",
        "  return device\n",
        "#构造联邦数据集\n",
        "federated_train_data = [get_data(ordered_mnist_train,i,j) for i in range(10) for j in range(10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IX3j3NpOWPV",
        "outputId": "2ee082fe-2239-4539-80fb-93533a0d77cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "source": [
        "#定义模型\n",
        "BATCH_SPEC = collections.OrderedDict(\n",
        "    x=tf.TensorSpec(shape=[None, 784], dtype=tf.float32),\n",
        "    y=tf.TensorSpec(shape=[None], dtype=tf.int32))\n",
        "BATCH_TYPE = tff.to_type(BATCH_SPEC)\n",
        "MODEL_SPEC = collections.OrderedDict(\n",
        "    weights=tf.TensorSpec(shape=[784, 10], dtype=tf.float32),\n",
        "    bias=tf.TensorSpec(shape=[10], dtype=tf.float32))\n",
        "MODEL_TYPE = tff.to_type(MODEL_SPEC)\n",
        "\n",
        "import random\n",
        "@tf.function\n",
        "def forward_pass(model, batch):\n",
        "  #L2正则项\n",
        "  loss=10e-4*tf.reduce_sum(model['weights']*model['weights'])+tf.reduce_sum(model['bias']*model['bias'])/(2*BATCH_SIZE)\n",
        "  #(100, 784) *(784, 10)+ (10,)=(100,10) 即100个预测值\n",
        "  predicted_y = tf.nn.softmax(\n",
        "      tf.matmul(batch['x'], model['weights']) + model['bias'])\n",
        "  #返回这一整个batch的损失值\n",
        "  return tf.reduce_mean(\n",
        "      #记录100个图片的损失值\n",
        "          tf.nn.softmax_cross_entropy_with_logits(\n",
        "          #tf.math.log(predicted_y)是100*10矩阵\n",
        "          #对应元素相乘\n",
        "          tf.one_hot(batch['y'], 10),tf.math.log(predicted_y)))+loss\n",
        "\n",
        "#批训练\n",
        "@tff.tf_computation(MODEL_TYPE, BATCH_TYPE)\n",
        "def batch_loss(model, batch):\n",
        "  return forward_pass(model, batch)\n",
        "\n",
        "@tff.tf_computation(MODEL_TYPE, BATCH_TYPE, tf.float32)\n",
        "def batch_train(initial_model, batch, learning_rate):\n",
        "  # Define a group of model variables and set them to `initial_model`. Must\n",
        "  # be defined outside the @tf.function.\n",
        "  model_vars = collections.OrderedDict([\n",
        "      (name, tf.Variable(name=name, initial_value=value))\n",
        "      for name, value in initial_model.items()\n",
        "  ])\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "  @tf.function\n",
        "  def _train_on_batch(model_vars, batch):\n",
        "    # Perform one step of gradient descent using loss from `batch_loss`.\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = forward_pass(model_vars, batch)\n",
        "    grads = tape.gradient(loss, model_vars)\n",
        "    optimizer.apply_gradients(\n",
        "        zip(tf.nest.flatten(grads), tf.nest.flatten(model_vars)))\n",
        "    return model_vars\n",
        "\n",
        "  return _train_on_batch(model_vars, batch)\n",
        "\n",
        "#设备训练\n",
        "LOCAL_DATA_TYPE = tff.SequenceType(BATCH_TYPE)\n",
        "E=1\n",
        "@tff.federated_computation(MODEL_TYPE, tf.float32, LOCAL_DATA_TYPE)\n",
        "def local_train(initial_model, learning_rate, all_batches):\n",
        "\n",
        "  # Mapping function to apply to each batch.\n",
        "  @tff.federated_computation(MODEL_TYPE, BATCH_TYPE)\n",
        "  def batch_fn(model, batch):\n",
        "    for _ in range(E):\n",
        "      model=batch_train(model, batch, learning_rate)\n",
        "    return model\n",
        "\n",
        "  return tff.sequence_reduce(all_batches, initial_model, batch_fn)\n",
        "\n",
        "#设备评估\n",
        "@tff.federated_computation(MODEL_TYPE, LOCAL_DATA_TYPE)\n",
        "def local_eval(model, all_batches):\n",
        "  # TODO(b/120157713): Replace with `tff.sequence_average()` once implemented.\n",
        "  return tff.sequence_sum(\n",
        "      tff.sequence_map(\n",
        "          tff.federated_computation(lambda b: batch_loss(model, b), BATCH_TYPE),\n",
        "          all_batches))\n",
        "\n",
        "#联邦评估\n",
        "SERVER_MODEL_TYPE = tff.type_at_server(MODEL_TYPE)\n",
        "CLIENT_DATA_TYPE = tff.type_at_clients(LOCAL_DATA_TYPE)\n",
        "@tff.federated_computation(SERVER_MODEL_TYPE, CLIENT_DATA_TYPE)\n",
        "def federated_eval(model, data):\n",
        "  return tff.federated_mean(\n",
        "      tff.federated_map(local_eval, [tff.federated_broadcast(model), data]))\n",
        "  \n",
        "\n",
        "#联邦训练\n",
        "SERVER_FLOAT_TYPE = tff.type_at_server(tf.float32)\n",
        "\n",
        "@tff.federated_computation(SERVER_MODEL_TYPE, SERVER_FLOAT_TYPE,\n",
        "                           CLIENT_DATA_TYPE,tff.type_at_clients(tf.float32))\n",
        "def federated_train(model, learning_rate, data,w):\n",
        "  return tff.federated_mean(\n",
        "      tff.federated_map(local_train, [\n",
        "          tff.federated_broadcast(model),\n",
        "          tff.federated_broadcast(learning_rate), data\n",
        "      ]),w)\n",
        "  \n",
        "  \n",
        "#计算准确率\n",
        "def accur(model):\n",
        "  device_index=range(1000)\n",
        "  device=[]\n",
        "  device.append({\n",
        "      'x':\n",
        "          np.array([mnist_test[0][i].flatten() / 255.0 for i in device_index],\n",
        "                  dtype=np.float32),\n",
        "      'y':\n",
        "          np.array([mnist_test[1][i] for i in device_index], dtype=np.int32)\n",
        "  })\n",
        "  predicted_y = tf.nn.softmax(\n",
        "        tf.matmul(device[0]['x'], model['weights']) + model['bias'])\n",
        "  p=[]\n",
        "  for i in predicted_y:\n",
        "    max=-1\n",
        "    flag=0\n",
        "    for j in range(1,10):\n",
        "      if i[j]>max:\n",
        "        max=i[j]\n",
        "        flag=j\n",
        "    p.append(flag)\n",
        "  cnt=0\n",
        "  for i in range(1000):\n",
        "    if p[i]==device[0]['y'][i]:\n",
        "      cnt+=1\n",
        "  return cnt/1000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function forward_pass at 0x7fc04aaf4ae8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module 'gast' has no attribute 'Index'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function forward_pass at 0x7fc04aaf4ae8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module 'gast' has no attribute 'Index'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbqs99OIOsFE",
        "outputId": "9f8be68c-2e77-45f2-e358-5f1a474e908b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "initial_model = collections.OrderedDict(\n",
        "    weights=np.zeros([784, 10], dtype=np.float32),\n",
        "    bias=np.zeros([10], dtype=np.float32))\n",
        "#设置权值\n",
        "weight_list = [1.9,0.1]\n",
        "for _ in range(98):\n",
        "  weight_list.append(1)\n",
        "weight_list = [x/100 for x in weight_list]\n",
        "model = initial_model\n",
        "for round_num in range(300):\n",
        "  learning_rate = 0.1 /(1+round_num)\n",
        "  model = federated_train(model, learning_rate, federated_train_data,weight_list)\n",
        "  loss = federated_eval(model, federated_train_data)\n",
        "  acc = accur(model)\n",
        "  print('round {}, loss={}, acc={}%'.format(round_num, loss, acc*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round 0, loss=10.372981071472168, acc=70.39999999999999%\n",
            "round 1, loss=9.729572296142578, acc=71.8%\n",
            "round 2, loss=9.276639938354492, acc=71.8%\n",
            "round 3, loss=8.937175750732422, acc=71.8%\n",
            "round 4, loss=8.671344757080078, acc=71.8%\n",
            "round 5, loss=8.455917358398438, acc=71.8%\n",
            "round 6, loss=8.27657699584961, acc=71.89999999999999%\n",
            "round 7, loss=8.124074935913086, acc=71.8%\n",
            "round 8, loss=7.992163181304932, acc=71.8%\n",
            "round 9, loss=7.876453399658203, acc=71.8%\n",
            "round 10, loss=7.773778915405273, acc=71.8%\n",
            "round 11, loss=7.681778430938721, acc=71.89999999999999%\n",
            "round 12, loss=7.598653793334961, acc=71.89999999999999%\n",
            "round 13, loss=7.52301025390625, acc=72.0%\n",
            "round 14, loss=7.453742027282715, acc=72.0%\n",
            "round 15, loss=7.389968395233154, acc=72.0%\n",
            "round 16, loss=7.330965995788574, acc=72.0%\n",
            "round 17, loss=7.276141166687012, acc=72.0%\n",
            "round 18, loss=7.224999904632568, acc=72.0%\n",
            "round 19, loss=7.1771321296691895, acc=71.89999999999999%\n",
            "round 20, loss=7.132178783416748, acc=71.8%\n",
            "round 21, loss=7.0898518562316895, acc=71.89999999999999%\n",
            "round 22, loss=7.049884796142578, acc=72.1%\n",
            "round 23, loss=7.012059211730957, acc=72.1%\n",
            "round 24, loss=6.976179122924805, acc=72.1%\n",
            "round 25, loss=6.942073822021484, acc=72.1%\n",
            "round 26, loss=6.9095964431762695, acc=72.1%\n",
            "round 27, loss=6.878612518310547, acc=72.1%\n",
            "round 28, loss=6.849008560180664, acc=72.1%\n",
            "round 29, loss=6.820674419403076, acc=72.1%\n",
            "round 30, loss=6.793519973754883, acc=72.1%\n",
            "round 31, loss=6.767460823059082, acc=72.1%\n",
            "round 32, loss=6.742420673370361, acc=72.1%\n",
            "round 33, loss=6.718331336975098, acc=72.1%\n",
            "round 34, loss=6.695131301879883, acc=72.1%\n",
            "round 35, loss=6.672764778137207, acc=72.2%\n",
            "round 36, loss=6.651175498962402, acc=72.2%\n",
            "round 37, loss=6.6303229331970215, acc=72.2%\n",
            "round 38, loss=6.610158920288086, acc=72.3%\n",
            "round 39, loss=6.590648651123047, acc=72.3%\n",
            "round 40, loss=6.571752548217773, acc=72.3%\n",
            "round 41, loss=6.553436279296875, acc=72.3%\n",
            "round 42, loss=6.535670757293701, acc=72.3%\n",
            "round 43, loss=6.518428325653076, acc=72.3%\n",
            "round 44, loss=6.50167989730835, acc=72.39999999999999%\n",
            "round 45, loss=6.485401153564453, acc=72.39999999999999%\n",
            "round 46, loss=6.469570159912109, acc=72.39999999999999%\n",
            "round 47, loss=6.454165458679199, acc=72.39999999999999%\n",
            "round 48, loss=6.439167022705078, acc=72.39999999999999%\n",
            "round 49, loss=6.424554347991943, acc=72.3%\n",
            "round 50, loss=6.410314559936523, acc=72.3%\n",
            "round 51, loss=6.396425724029541, acc=72.3%\n",
            "round 52, loss=6.382877349853516, acc=72.3%\n",
            "round 53, loss=6.369650840759277, acc=72.39999999999999%\n",
            "round 54, loss=6.356736660003662, acc=72.39999999999999%\n",
            "round 55, loss=6.344120025634766, acc=72.39999999999999%\n",
            "round 56, loss=6.331789493560791, acc=72.39999999999999%\n",
            "round 57, loss=6.319730758666992, acc=72.39999999999999%\n",
            "round 58, loss=6.307937145233154, acc=72.39999999999999%\n",
            "round 59, loss=6.296399116516113, acc=72.39999999999999%\n",
            "round 60, loss=6.285103797912598, acc=72.39999999999999%\n",
            "round 61, loss=6.274044036865234, acc=72.5%\n",
            "round 62, loss=6.26320743560791, acc=72.5%\n",
            "round 63, loss=6.252591609954834, acc=72.5%\n",
            "round 64, loss=6.242187023162842, acc=72.5%\n",
            "round 65, loss=6.231985569000244, acc=72.5%\n",
            "round 66, loss=6.221979141235352, acc=72.5%\n",
            "round 67, loss=6.212162971496582, acc=72.5%\n",
            "round 68, loss=6.2025299072265625, acc=72.6%\n",
            "round 69, loss=6.193075656890869, acc=72.6%\n",
            "round 70, loss=6.1837897300720215, acc=72.6%\n",
            "round 71, loss=6.174671649932861, acc=72.5%\n",
            "round 72, loss=6.165714740753174, acc=72.5%\n",
            "round 73, loss=6.156914710998535, acc=72.39999999999999%\n",
            "round 74, loss=6.1482648849487305, acc=72.5%\n",
            "round 75, loss=6.1397600173950195, acc=72.5%\n",
            "round 76, loss=6.1313982009887695, acc=72.5%\n",
            "round 77, loss=6.123175621032715, acc=72.6%\n",
            "round 78, loss=6.115087509155273, acc=72.6%\n",
            "round 79, loss=6.107127666473389, acc=72.6%\n",
            "round 80, loss=6.099295139312744, acc=72.6%\n",
            "round 81, loss=6.0915846824646, acc=72.6%\n",
            "round 82, loss=6.083992958068848, acc=72.6%\n",
            "round 83, loss=6.076519012451172, acc=72.6%\n",
            "round 84, loss=6.069157123565674, acc=72.6%\n",
            "round 85, loss=6.061906337738037, acc=72.6%\n",
            "round 86, loss=6.0547614097595215, acc=72.6%\n",
            "round 87, loss=6.0477213859558105, acc=72.6%\n",
            "round 88, loss=6.040783882141113, acc=72.6%\n",
            "round 89, loss=6.033944129943848, acc=72.6%\n",
            "round 90, loss=6.027200698852539, acc=72.6%\n",
            "round 91, loss=6.020553112030029, acc=72.6%\n",
            "round 92, loss=6.013996124267578, acc=72.6%\n",
            "round 93, loss=6.007532119750977, acc=72.6%\n",
            "round 94, loss=6.001150608062744, acc=72.6%\n",
            "round 95, loss=5.9948554039001465, acc=72.6%\n",
            "round 96, loss=5.988643646240234, acc=72.6%\n",
            "round 97, loss=5.982513427734375, acc=72.6%\n",
            "round 98, loss=5.976461887359619, acc=72.6%\n",
            "round 99, loss=5.970491886138916, acc=72.6%\n",
            "round 100, loss=5.964592456817627, acc=72.6%\n",
            "round 101, loss=5.958770751953125, acc=72.6%\n",
            "round 102, loss=5.953021049499512, acc=72.6%\n",
            "round 103, loss=5.947341442108154, acc=72.6%\n",
            "round 104, loss=5.941730976104736, acc=72.6%\n",
            "round 105, loss=5.936187267303467, acc=72.6%\n",
            "round 106, loss=5.930712699890137, acc=72.6%\n",
            "round 107, loss=5.9253010749816895, acc=72.6%\n",
            "round 108, loss=5.919955253601074, acc=72.6%\n",
            "round 109, loss=5.914668560028076, acc=72.6%\n",
            "round 110, loss=5.909443378448486, acc=72.6%\n",
            "round 111, loss=5.904280185699463, acc=72.6%\n",
            "round 112, loss=5.899175643920898, acc=72.6%\n",
            "round 113, loss=5.894125461578369, acc=72.7%\n",
            "round 114, loss=5.889133453369141, acc=72.7%\n",
            "round 115, loss=5.884196758270264, acc=72.7%\n",
            "round 116, loss=5.8793134689331055, acc=72.7%\n",
            "round 117, loss=5.874481678009033, acc=72.7%\n",
            "round 118, loss=5.869705677032471, acc=72.7%\n",
            "round 119, loss=5.8649773597717285, acc=72.7%\n",
            "round 120, loss=5.860297203063965, acc=72.7%\n",
            "round 121, loss=5.855671405792236, acc=72.7%\n",
            "round 122, loss=5.851091384887695, acc=72.7%\n",
            "round 123, loss=5.846558094024658, acc=72.7%\n",
            "round 124, loss=5.842071056365967, acc=72.7%\n",
            "round 125, loss=5.837630748748779, acc=72.7%\n",
            "round 126, loss=5.833232879638672, acc=72.7%\n",
            "round 127, loss=5.8288798332214355, acc=72.7%\n",
            "round 128, loss=5.824571132659912, acc=72.7%\n",
            "round 129, loss=5.820303916931152, acc=72.7%\n",
            "round 130, loss=5.816077709197998, acc=72.7%\n",
            "round 131, loss=5.811893939971924, acc=72.7%\n",
            "round 132, loss=5.807748794555664, acc=72.7%\n",
            "round 133, loss=5.803645610809326, acc=72.7%\n",
            "round 134, loss=5.799578666687012, acc=72.7%\n",
            "round 135, loss=5.7955498695373535, acc=72.7%\n",
            "round 136, loss=5.791559219360352, acc=72.7%\n",
            "round 137, loss=5.787606239318848, acc=72.7%\n",
            "round 138, loss=5.783689022064209, acc=72.7%\n",
            "round 139, loss=5.779806613922119, acc=72.7%\n",
            "round 140, loss=5.775959491729736, acc=72.7%\n",
            "round 141, loss=5.7721476554870605, acc=72.8%\n",
            "round 142, loss=5.768371105194092, acc=72.8%\n",
            "round 143, loss=5.764625072479248, acc=72.8%\n",
            "round 144, loss=5.760915756225586, acc=72.8%\n",
            "round 145, loss=5.7572340965271, acc=72.8%\n",
            "round 146, loss=5.753586292266846, acc=72.8%\n",
            "round 147, loss=5.749969959259033, acc=72.8%\n",
            "round 148, loss=5.746387958526611, acc=72.8%\n",
            "round 149, loss=5.742830753326416, acc=72.8%\n",
            "round 150, loss=5.739307880401611, acc=72.8%\n",
            "round 151, loss=5.73581075668335, acc=72.8%\n",
            "round 152, loss=5.7323479652404785, acc=72.8%\n",
            "round 153, loss=5.728910446166992, acc=72.8%\n",
            "round 154, loss=5.7255024909973145, acc=72.8%\n",
            "round 155, loss=5.72212028503418, acc=72.8%\n",
            "round 156, loss=5.718766689300537, acc=72.8%\n",
            "round 157, loss=5.715438365936279, acc=72.8%\n",
            "round 158, loss=5.712139129638672, acc=72.8%\n",
            "round 159, loss=5.708866596221924, acc=72.8%\n",
            "round 160, loss=5.705617904663086, acc=72.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muC6lh09Vluy"
      },
      "source": [
        "round 0, loss=21.60552406311035\n",
        "round 1, loss=20.365678787231445\n",
        "round 2, loss=19.27480125427246\n",
        "round 3, loss=18.31110954284668\n",
        "round 4, loss=17.457256317138672\n",
        "\n",
        "不改权值的结果：\n",
        "```\n",
        "round 0, loss=10.36648941040039, acc=70.3%\n",
        "round 1, loss=9.720335960388184, acc=71.39999999999999%\n",
        "round 2, loss=9.265982627868652, acc=71.5%\n",
        "round 3, loss=8.925678253173828, acc=71.5%\n",
        "round 4, loss=8.659313201904297, acc=71.6%\n",
        "round 5, loss=8.443524360656738, acc=71.5%\n",
        "round 6, loss=8.263936042785645, acc=71.7%\n",
        "round 7, loss=8.1112642288208, acc=71.6%\n",
        "round 8, loss=7.979235649108887, acc=71.6%\n",
        "round 9, loss=7.8634490966796875, acc=71.6%\n",
        "round 10, loss=7.760721206665039, acc=71.7%\n",
        "round 11, loss=7.668695449829102, acc=71.7%\n",
        "round 12, loss=7.585556507110596, acc=71.8%\n",
        "round 13, loss=7.509913921356201, acc=71.8%\n",
        "round 14, loss=7.4406561851501465, acc=71.8%\n",
        "round 15, loss=7.376895904541016, acc=71.8%\n",
        "round 16, loss=7.317915439605713, acc=71.8%\n",
        "round 17, loss=7.263117790222168, acc=71.8%\n",
        "round 18, loss=7.212005615234375, acc=71.8%\n",
        "round 19, loss=7.164167404174805, acc=71.8%\n",
        "round 20, loss=7.119253635406494, acc=71.7%\n",
        "round 21, loss=7.076962471008301, acc=71.89999999999999%\n",
        "round 22, loss=7.037031650543213, acc=72.0%\n",
        "round 23, loss=6.999242782592773, acc=72.0%\n",
        "round 24, loss=6.963403701782227, acc=72.0%\n",
        "round 25, loss=6.929336071014404, acc=72.0%\n",
        "round 26, loss=6.896897792816162, acc=72.0%\n",
        "round 27, loss=6.865954399108887, acc=72.0%\n",
        "round 28, loss=6.836390495300293, acc=72.0%\n",
        "round 29, loss=6.808094501495361, acc=72.0%\n",
        "round 30, loss=6.780981063842773, acc=72.0%\n",
        "round 31, loss=6.754961013793945, acc=72.0%\n",
        "round 32, loss=6.729955673217773, acc=72.0%\n",
        "round 33, loss=6.705905914306641, acc=72.0%\n",
        "round 34, loss=6.682745933532715, acc=72.0%\n",
        "round 35, loss=6.660414218902588, acc=72.0%\n",
        "round 36, loss=6.638864040374756, acc=72.0%\n",
        "round 37, loss=6.618048667907715, acc=72.0%\n",
        "round 38, loss=6.5979228019714355, acc=72.1%\n",
        "round 39, loss=6.578448295593262, acc=72.1%\n",
        "round 40, loss=6.559586048126221, acc=72.1%\n",
        "round 41, loss=6.541306972503662, acc=72.1%\n",
        "round 42, loss=6.523574352264404, acc=72.1%\n",
        "round 43, loss=6.5063652992248535, acc=72.1%\n",
        "round 44, loss=6.489650726318359, acc=72.2%\n",
        "round 45, loss=6.473405838012695, acc=72.2%\n",
        "round 46, loss=6.457608699798584, acc=72.2%\n",
        "round 47, loss=6.442233085632324, acc=72.2%\n",
        "round 48, loss=6.427267551422119, acc=72.2%\n",
        "round 49, loss=6.412689208984375, acc=72.1%\n",
        "round 50, loss=6.398477077484131, acc=72.1%\n",
        "round 51, loss=6.384622573852539, acc=72.1%\n",
        "round 52, loss=6.371103286743164, acc=72.1%\n",
        "round 53, loss=6.357907772064209, acc=72.1%\n",
        "round 54, loss=6.345022201538086, acc=72.2%\n",
        "round 55, loss=6.332433700561523, acc=72.2%\n",
        "round 56, loss=6.320131301879883, acc=72.2%\n",
        "round 57, loss=6.308100700378418, acc=72.2%\n",
        "round 58, loss=6.296338081359863, acc=72.2%\n",
        "round 59, loss=6.284825325012207, acc=72.3%\n",
        "round 60, loss=6.273555755615234, acc=72.3%\n",
        "round 61, loss=6.262521266937256, acc=72.3%\n",
        "round 62, loss=6.251714706420898, acc=72.3%\n",
        "round 63, loss=6.241124153137207, acc=72.3%\n",
        "round 64, loss=6.230745315551758, acc=72.3%\n",
        "round 65, loss=6.220568656921387, acc=72.3%\n",
        "round 66, loss=6.210587978363037, acc=72.3%\n",
        "round 67, loss=6.200796127319336, acc=72.3%\n",
        "round 68, loss=6.191188335418701, acc=72.3%\n",
        "round 69, loss=6.181758403778076, acc=72.3%\n",
        "round 70, loss=6.172497749328613, acc=72.1%\n",
        "round 71, loss=6.16340446472168, acc=72.1%\n",
        "round 72, loss=6.154470920562744, acc=72.1%\n",
        "round 73, loss=6.145693778991699, acc=72.2%\n",
        "round 74, loss=6.137065887451172, acc=72.2%\n",
        "round 75, loss=6.128585338592529, acc=72.3%\n",
        "round 76, loss=6.120245456695557, acc=72.3%\n",
        "round 77, loss=6.1120452880859375, acc=72.3%\n",
        "round 78, loss=6.103979110717773, acc=72.3%\n",
        "round 79, loss=6.09604024887085, acc=72.3%\n",
        "round 80, loss=6.088228702545166, acc=72.3%\n",
        "round 81, loss=6.080538749694824, acc=72.3%\n",
        "round 82, loss=6.072969913482666, acc=72.3%\n",
        "round 83, loss=6.065515518188477, acc=72.3%\n",
        "round 84, loss=6.058176040649414, acc=72.3%\n",
        "round 85, loss=6.050944805145264, acc=72.3%\n",
        "round 86, loss=6.043820858001709, acc=72.3%\n",
        "round 87, loss=6.036800384521484, acc=72.3%\n",
        "round 88, loss=6.029882907867432, acc=72.3%\n",
        "round 89, loss=6.023061752319336, acc=72.3%\n",
        "round 90, loss=6.016338348388672, acc=72.3%\n",
        "round 91, loss=6.009708881378174, acc=72.3%\n",
        "round 92, loss=6.003171920776367, acc=72.3%\n",
        "round 93, loss=5.996725082397461, acc=72.3%\n",
        "round 94, loss=5.9903645515441895, acc=72.3%\n",
        "round 95, loss=5.984086513519287, acc=72.3%\n",
        "round 96, loss=5.977894306182861, acc=72.3%\n",
        "round 97, loss=5.971782684326172, acc=72.3%\n",
        "round 98, loss=5.965747833251953, acc=72.3%\n",
        "round 99, loss=5.959793567657471, acc=72.3%\n",
        "round 100, loss=5.953916072845459, acc=72.3%\n",
        "round 101, loss=5.948107719421387, acc=72.3%\n",
        "round 102, loss=5.942374229431152, acc=72.3%\n",
        "round 103, loss=5.936712741851807, acc=72.3%\n",
        "round 104, loss=5.9311203956604, acc=72.3%\n",
        "round 105, loss=5.925593852996826, acc=72.3%\n",
        "round 106, loss=5.920135974884033, acc=72.3%\n",
        "round 107, loss=5.914740085601807, acc=72.3%\n",
        "round 108, loss=5.9094109535217285, acc=72.3%\n",
        "round 109, loss=5.904142379760742, acc=72.3%\n",
        "round 110, loss=5.898932933807373, acc=72.3%\n",
        "round 111, loss=5.8937859535217285, acc=72.3%\n",
        "round 112, loss=5.888695240020752, acc=72.39999999999999%\n",
        "round 113, loss=5.883661270141602, acc=72.39999999999999%\n",
        "round 114, loss=5.8786845207214355, acc=72.39999999999999%\n",
        "round 115, loss=5.873762607574463, acc=72.39999999999999%\n",
        "round 116, loss=5.868896007537842, acc=72.5%\n",
        "round 117, loss=5.864081382751465, acc=72.5%\n",
        "round 118, loss=5.859317779541016, acc=72.5%\n",
        "round 119, loss=5.8546037673950195, acc=72.5%\n",
        "round 120, loss=5.849942684173584, acc=72.5%\n",
        "round 121, loss=5.845328330993652, acc=72.5%\n",
        "round 122, loss=5.840763568878174, acc=72.5%\n",
        "round 123, loss=5.836244106292725, acc=72.5%\n",
        "round 124, loss=5.831772327423096, acc=72.5%\n",
        "round 125, loss=5.827345371246338, acc=72.5%\n",
        "round 126, loss=5.822963237762451, acc=72.5%\n",
        "round 127, loss=5.818624496459961, acc=72.5%\n",
        "round 128, loss=5.8143310546875, acc=72.5%\n",
        "round 129, loss=5.8100762367248535, acc=72.5%\n",
        "round 130, loss=5.805863857269287, acc=72.5%\n",
        "round 131, loss=5.801693916320801, acc=72.5%\n",
        "round 132, loss=5.7975640296936035, acc=72.5%\n",
        "round 133, loss=5.79347038269043, acc=72.5%\n",
        "round 134, loss=5.789417743682861, acc=72.5%\n",
        "round 135, loss=5.785403251647949, acc=72.5%\n",
        "round 136, loss=5.781425952911377, acc=72.5%\n",
        "round 137, loss=5.7774858474731445, acc=72.5%\n",
        "round 138, loss=5.7735819816589355, acc=72.5%\n",
        "round 139, loss=5.769712448120117, acc=72.5%\n",
        "round 140, loss=5.765878200531006, acc=72.6%\n",
        "round 141, loss=5.762081146240234, acc=72.6%\n",
        "round 142, loss=5.7583136558532715, acc=72.6%\n",
        "round 143, loss=5.754583358764648, acc=72.6%\n",
        "round 144, loss=5.750883102416992, acc=72.6%\n",
        "round 145, loss=5.747217178344727, acc=72.6%\n",
        "round 146, loss=5.743582248687744, acc=72.6%\n",
        "round 147, loss=5.7399773597717285, acc=72.6%\n",
        "round 148, loss=5.736404895782471, acc=72.6%\n",
        "round 149, loss=5.732864856719971, acc=72.6%\n",
        "round 150, loss=5.729351043701172, acc=72.6%\n",
        "round 151, loss=5.7258687019348145, acc=72.6%\n",
        "round 152, loss=5.722413539886475, acc=72.6%\n",
        "round 153, loss=5.718986988067627, acc=72.6%\n",
        "round 154, loss=5.7155914306640625, acc=72.6%\n",
        "round 155, loss=5.712220668792725, acc=72.6%\n",
        "round 156, loss=5.7088799476623535, acc=72.6%\n",
        "round 157, loss=5.705564498901367, acc=72.6%\n",
        "round 158, loss=5.702276229858398, acc=72.6%\n",
        "round 159, loss=5.699013710021973, acc=72.6%\n",
        "round 160, loss=5.695777416229248, acc=72.5%\n",
        "round 161, loss=5.69256591796875, acc=72.5%\n",
        "round 162, loss=5.689380645751953, acc=72.5%\n",
        "round 163, loss=5.68621826171875, acc=72.5%\n",
        "round 164, loss=5.683082103729248, acc=72.5%\n",
        "round 165, loss=5.679969310760498, acc=72.5%\n",
        "round 166, loss=5.676881313323975, acc=72.5%\n",
        "round 167, loss=5.673816680908203, acc=72.5%\n",
        "round 168, loss=5.670772075653076, acc=72.5%\n",
        "round 169, loss=5.667752742767334, acc=72.5%\n",
        "round 170, loss=5.664754867553711, acc=72.5%\n",
        "round 171, loss=5.6617817878723145, acc=72.5%\n",
        "round 172, loss=5.6588263511657715, acc=72.5%\n",
        "round 173, loss=5.655895233154297, acc=72.5%\n",
        "round 174, loss=5.65298318862915, acc=72.5%\n",
        "round 175, loss=5.650094985961914, acc=72.6%\n",
        "round 176, loss=5.647225856781006, acc=72.6%\n",
        "round 177, loss=5.6443772315979, acc=72.6%\n",
        "round 178, loss=5.6415510177612305, acc=72.6%\n",
        "round 179, loss=5.638741493225098, acc=72.6%\n",
        "round 180, loss=5.6359543800354, acc=72.6%\n",
        "round 181, loss=5.633185386657715, acc=72.6%\n",
        "round 182, loss=5.630435943603516, acc=72.6%\n",
        "round 183, loss=5.627706527709961, acc=72.6%\n",
        "round 184, loss=5.624994277954102, acc=72.6%\n",
        "round 185, loss=5.6222991943359375, acc=72.6%\n",
        "round 186, loss=5.619626522064209, acc=72.6%\n",
        "round 187, loss=5.616968154907227, acc=72.6%\n",
        "round 188, loss=5.614329814910889, acc=72.6%\n",
        "round 189, loss=5.611709117889404, acc=72.6%\n",
        "round 190, loss=5.609104156494141, acc=72.6%\n",
        "round 191, loss=5.606518745422363, acc=72.6%\n",
        "round 192, loss=5.603947639465332, acc=72.6%\n",
        "round 193, loss=5.6013946533203125, acc=72.6%\n",
        "round 194, loss=5.598859786987305, acc=72.6%\n",
        "round 195, loss=5.596339225769043, acc=72.6%\n",
        "round 196, loss=5.593835353851318, acc=72.6%\n",
        "round 197, loss=5.591347694396973, acc=72.6%\n",
        "round 198, loss=5.588876724243164, acc=72.6%\n",
        "round 199, loss=5.586421012878418, acc=72.6%\n",
        "round 200, loss=5.583980083465576, acc=72.6%\n",
        "round 201, loss=5.581554412841797, acc=72.6%\n",
        "round 202, loss=5.579144477844238, acc=72.6%\n",
        "round 203, loss=5.576749801635742, acc=72.6%\n",
        "round 204, loss=5.57436990737915, acc=72.6%\n",
        "round 205, loss=5.5720014572143555, acc=72.6%\n",
        "round 206, loss=5.569650173187256, acc=72.6%\n",
        "round 207, loss=5.567317008972168, acc=72.6%\n",
        "round 208, loss=5.5649919509887695, acc=72.6%\n",
        "round 209, loss=5.562684535980225, acc=72.6%\n",
        "round 210, loss=5.56038761138916, acc=72.6%\n",
        "round 211, loss=5.558109283447266, acc=72.6%\n",
        "round 212, loss=5.555840492248535, acc=72.6%\n",
        "round 213, loss=5.553586483001709, acc=72.6%\n",
        "round 214, loss=5.5513458251953125, acc=72.6%\n",
        "round 215, loss=5.549118518829346, acc=72.6%\n",
        "round 216, loss=5.546905040740967, acc=72.6%\n",
        "round 217, loss=5.544701099395752, acc=72.6%\n",
        "round 218, loss=5.542513847351074, acc=72.6%\n",
        "round 219, loss=5.540337085723877, acc=72.6%\n",
        "round 220, loss=5.538171291351318, acc=72.6%\n",
        "round 221, loss=5.5360212326049805, acc=72.6%\n",
        "round 222, loss=5.533881664276123, acc=72.6%\n",
        "round 223, loss=5.531755447387695, acc=72.6%\n",
        "round 224, loss=5.529639720916748, acc=72.6%\n",
        "round 225, loss=5.527536392211914, acc=72.6%\n",
        "round 226, loss=5.525442123413086, acc=72.6%\n",
        "round 227, loss=5.52336311340332, acc=72.6%\n",
        "round 228, loss=5.521296977996826, acc=72.6%\n",
        "round 229, loss=5.519237041473389, acc=72.6%\n",
        "round 230, loss=5.517190933227539, acc=72.6%\n",
        "round 231, loss=5.515157699584961, acc=72.6%\n",
        "round 232, loss=5.513134002685547, acc=72.6%\n",
        "round 233, loss=5.511120796203613, acc=72.7%\n",
        "round 234, loss=5.509117603302002, acc=72.7%\n",
        "round 235, loss=5.507127285003662, acc=72.7%\n",
        "round 236, loss=5.505147933959961, acc=72.7%\n",
        "round 237, loss=5.503175258636475, acc=72.7%\n",
        "round 238, loss=5.501216411590576, acc=72.89999999999999%\n",
        "round 239, loss=5.499266147613525, acc=72.89999999999999%\n",
        "round 240, loss=5.49732780456543, acc=72.89999999999999%\n",
        "round 241, loss=5.49539852142334, acc=72.89999999999999%\n",
        "round 242, loss=5.493480205535889, acc=72.89999999999999%\n",
        "round 243, loss=5.491570949554443, acc=72.89999999999999%\n",
        "round 244, loss=5.489671230316162, acc=72.89999999999999%\n",
        "round 245, loss=5.4877824783325195, acc=72.89999999999999%\n",
        "round 246, loss=5.485902786254883, acc=72.89999999999999%\n",
        "round 247, loss=5.4840312004089355, acc=72.89999999999999%\n",
        "round 248, loss=5.482170581817627, acc=72.89999999999999%\n",
        "round 249, loss=5.480320453643799, acc=72.89999999999999%\n",
        "round 250, loss=5.47847843170166, acc=72.89999999999999%\n",
        "round 251, loss=5.476645469665527, acc=72.89999999999999%\n",
        "round 252, loss=5.474822521209717, acc=72.89999999999999%\n",
        "round 253, loss=5.473005771636963, acc=72.89999999999999%\n",
        "round 254, loss=5.4712018966674805, acc=72.89999999999999%\n",
        "round 255, loss=5.469404220581055, acc=72.89999999999999%\n",
        "round 256, loss=5.467617988586426, acc=72.89999999999999%\n",
        "round 257, loss=5.4658379554748535, acc=72.89999999999999%\n",
        "round 258, loss=5.4640679359436035, acc=72.89999999999999%\n",
        "round 259, loss=5.462305068969727, acc=72.89999999999999%\n",
        "round 260, loss=5.460551738739014, acc=72.89999999999999%\n",
        "round 261, loss=5.458808422088623, acc=72.89999999999999%\n",
        "round 262, loss=5.457070827484131, acc=72.89999999999999%\n",
        "round 263, loss=5.4553422927856445, acc=72.89999999999999%\n",
        "round 264, loss=5.453622341156006, acc=72.89999999999999%\n",
        "round 265, loss=5.451910495758057, acc=72.89999999999999%\n",
        "round 266, loss=5.4502081871032715, acc=72.89999999999999%\n",
        "round 267, loss=5.448512554168701, acc=72.89999999999999%\n",
        "round 268, loss=5.446822643280029, acc=72.89999999999999%\n",
        "round 269, loss=5.445144176483154, acc=72.89999999999999%\n",
        "round 270, loss=5.443472385406494, acc=72.89999999999999%\n",
        "round 271, loss=5.441807746887207, acc=72.89999999999999%\n",
        "round 272, loss=5.440150737762451, acc=72.89999999999999%\n",
        "round 273, loss=5.438502311706543, acc=72.89999999999999%\n",
        "round 274, loss=5.436861515045166, acc=72.89999999999999%\n",
        "round 275, loss=5.435227870941162, acc=72.89999999999999%\n",
        "round 276, loss=5.433599948883057, acc=72.89999999999999%\n",
        "round 277, loss=5.431982040405273, acc=72.89999999999999%\n",
        "round 278, loss=5.430369853973389, acc=72.89999999999999%\n",
        "round 279, loss=5.428764820098877, acc=72.89999999999999%\n",
        "round 280, loss=5.427168369293213, acc=73.0%\n",
        "round 281, loss=5.425579071044922, acc=73.0%\n",
        "round 282, loss=5.423994064331055, acc=73.0%\n",
        "round 283, loss=5.422417163848877, acc=73.0%\n",
        "round 284, loss=5.420848846435547, acc=73.0%\n",
        "round 285, loss=5.419287204742432, acc=73.0%\n",
        "round 286, loss=5.417731761932373, acc=73.0%\n",
        "round 287, loss=5.4161834716796875, acc=73.0%\n",
        "round 288, loss=5.4146409034729, acc=73.2%\n",
        "round 289, loss=5.4131059646606445, acc=73.2%\n",
        "round 290, loss=5.411578178405762, acc=73.2%\n",
        "round 291, loss=5.410055637359619, acc=73.2%\n",
        "round 292, loss=5.40854024887085, acc=73.2%\n",
        "round 293, loss=5.40703010559082, acc=73.2%\n",
        "round 294, loss=5.405528545379639, acc=73.2%\n",
        "round 295, loss=5.404033184051514, acc=73.2%\n",
        "round 296, loss=5.402544021606445, acc=73.2%\n",
        "round 297, loss=5.401060104370117, acc=73.2%\n",
        "round 298, loss=5.399583339691162, acc=73.2%\n",
        "round 299, loss=5.398112773895264, acc=73.2%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUg7lHHAXHap"
      },
      "source": [
        "改变权值(0.19,0.01)后的结果\n",
        "```\n",
        "round 0, loss=10.372981071472168, acc=70.39999999999999%\n",
        "round 1, loss=9.729572296142578, acc=71.8%\n",
        "round 2, loss=9.276639938354492, acc=71.8%\n",
        "round 3, loss=8.937175750732422, acc=71.8%\n",
        "round 4, loss=8.671344757080078, acc=71.8%\n",
        "round 5, loss=8.455917358398438, acc=71.8%\n",
        "round 6, loss=8.27657699584961, acc=71.89999999999999%\n",
        "round 7, loss=8.124074935913086, acc=71.8%\n",
        "round 8, loss=7.992163181304932, acc=71.8%\n",
        "round 9, loss=7.876453399658203, acc=71.8%\n",
        "round 10, loss=7.773778915405273, acc=71.8%\n",
        "round 11, loss=7.681778430938721, acc=71.89999999999999%\n",
        "round 12, loss=7.598653793334961, acc=71.89999999999999%\n",
        "round 13, loss=7.52301025390625, acc=72.0%\n",
        "round 14, loss=7.453742027282715, acc=72.0%\n",
        "round 15, loss=7.389968395233154, acc=72.0%\n",
        "```"
      ]
    }
  ]
}